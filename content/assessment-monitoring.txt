# Assessment: Monitoring & Evaluation Plan
# Measuring implementation success and validating X→M→Y causal chain

## Logic Model: X → M → Y Causal Pathway

### X (Independent Variable): Standardized Performance Appraisal System (SPAS)

**Operational Definition:**
Standardized Performance Appraisal System (SPAS) consisting of five integrated components:
1. **Unified Rating Scale** – Organization-wide 5-point competency rubric aligned to job families
2. **Manager Training** – 8-hour certification covering bias awareness, documentation, and feedback skills
3. **Digital Platform** – Centralized appraisal system with templates, prompts, and audit trails
4. **Quarterly Feedback** – Structured check-ins supplementing annual reviews
5. **Bias Monitoring** – HR analytics dashboard tracking rating distributions by demographics/managers

**Implementation Fidelity Indicators (X Quality):**
- ✅ 100% managers complete 8-hour training certification
- ✅ ≥90% appraisals submitted via digital platform (not paper/email workarounds)
- ✅ ≥85% employees receive quarterly feedback sessions (not just annual)
- ✅ Bias dashboard reviewed monthly by leadership with documented action plans
- ✅ Rating scale used consistently (random audit shows ≥90% compliance with rubric definitions)

**Evidence Foundation for X:**
- Bayesian synthesis (98% confidence) identifies standardization as causal mechanism
- Organizational evidence (97% posterior, 600K+ sample) validates structured systems reduce legal risk
- Practitioner evidence (88% posterior) proves training is critical (r=0.25 with training vs. r=0.15 without)

---

### M (Mediating Variables): Appraisal Consistency & Perceived Fairness

**Mediator 1: Appraisal Consistency**
*"How does X (standardization) lead to Y (reduced legal risk)? Through M (consistency)."*

**Operational Definition:**
Statistical consistency of performance ratings across managers, departments, and demographic groups, measured by:
- **Rating distribution variance:** Standard deviation of average ratings across managers (lower = more consistent)
- **Inter-rater reliability:** Correlation between two managers' ratings of same employee competencies (higher = more consistent)
- **Demographic parity:** Chi-square test comparing rating distributions by race/gender/age (p>0.05 = no disparate impact)

**Why This Mediates X→Y:**
- Scientific evidence (75% posterior) shows 35-40% inconsistency creates legal vulnerability
- Organizational evidence (97% posterior, 81K+ EEOC charges) demonstrates inconsistent processes generate discrimination claims
- Standardization (X) → Reduces inconsistency (M) → Fewer legally indefensible decisions (Y)

**Measurement Strategy:**
- **Baseline:** Collect 12 months of pre-implementation rating data, calculate variance across managers
- **Target:** Reduce rating variance by 50% (e.g., from SD=0.80 to SD=0.40 across manager averages)
- **Timeline:** Measure quarterly during pilot; expect 25% improvement at 6 months, 50% at 12 months
- **Data Source:** HRIS performance management module, extract rating distributions by manager/department/demographics

**Evidence Hypothesis Test:**
- **H1:** Managers who complete training AND use digital platform will show lower rating variance than baseline (tests X→M path)
- **H2:** Departments with lower rating variance will have fewer employee complaints/charges (tests M→Y path)

---

**Mediator 2: Perceived Fairness**
*"How does M (consistency) lead to Y (outcomes)? Through employee perceptions of fairness."*

**Operational Definition:**
Employee perceptions that performance appraisals are fair, unbiased, and based on job-relevant criteria, measured by:
- **Procedural fairness:** "My performance appraisal process is fair and consistent" (5-point Likert scale)
- **Distributive fairness:** "My performance rating accurately reflects my contributions" (5-point Likert scale)
- **Trust in system:** "I trust that performance appraisals are free from bias" (5-point Likert scale)
- **Composite fairness score:** Average of 6 items (α reliability target >0.85)

**Why This Mediates M→Y:**
- Organizational evidence (97% posterior) shows r=0.54 correlation between perceived fairness and legal outcomes
- Employees who perceive unfairness are more likely to file complaints/charges (stakeholder evidence: 83% want fair processes)
- Consistency (M) → Improved fairness perceptions (M2) → Reduced legal actions (Y)

**Measurement Strategy:**
- **Baseline:** Pre-implementation employee survey (target 60% response rate, n=150 in pilot)
- **Target:** Increase fairness perceptions from 43% (FEVS benchmark) to 60% (+17 points) at 12 months
- **Timeline:** Quarterly pulse surveys (3 items) + comprehensive survey at 12 months (6 items)
- **Data Source:** Anonymous online survey via Qualtrics/SurveyMonkey, benchmarked to OPM FEVS items

**Evidence Hypothesis Test:**
- **H3:** Employees in pilot division will show greater fairness perception improvement than control group (tests causal attribution to X)
- **H4:** Fairness perceptions will correlate with actual rating consistency (tests M1→M2 path, expect r≥0.40)

---

### Y (Dependent Variable): Reduced Legal Exposure & Improved Organizational Outcomes

**Y1: Legal Risk Reduction (Primary Outcome)**

**Operational Definition:**
Reduced organizational exposure to performance appraisal-related legal claims, measured by:
- **Discrimination charges filed:** Annual count of EEOC/state charges citing performance appraisal as basis
- **Charge resolution costs:** Total settlement + defense costs for performance-related charges
- **Internal complaints:** Count of HR complaints alleging appraisal discrimination/unfairness
- **Litigation risk score:** Legal department assessment of pending charge severity (1-10 scale)

**Evidence-Based Targets:**
- **Primary target:** 20-30% reduction in performance-related discrimination charges (based on organizational evidence: 81K+ charges, 20-30% reduction with structured systems)
- **Secondary target:** 40% reduction in internal complaints (faster to change than external charges)
- **Tertiary target:** 25% reduction in average settlement/defense costs (better documentation = stronger defense)

**Measurement Strategy:**
- **Baseline:** 12-month pre-implementation period – count charges, track costs, document complaints
- **Timeline:** Recognize 12-18 month lag (charges filed reflect PAST practices, not current ones)
- **Data Source:** EEOC charge database (HR records), legal department case tracking, employee relations complaint log
- **Comparison:** Pilot division charge rate vs. non-pilot divisions (quasi-experimental design)

**Causal Inference Considerations:**
- ⚠️ **Confounding variables:** Economic conditions, industry trends, EEOC enforcement priorities
- ✅ **Temporal precedence:** Charges decline AFTER (not before) standardization implementation
- ✅ **Dose-response:** Divisions with higher implementation fidelity (X quality) show greater charge reduction
- ✅ **Alternative explanations:** Control for org-wide policy changes, turnover, restructuring

---

**Y2: Organizational Performance (Secondary Outcome)**

**Operational Definition:**
Improved talent management outcomes resulting from more accurate, consistent performance appraisals:
- **Rating validity:** Correlation between performance ratings and objective KPIs (sales, quality, productivity)
- **Turnover of high performers:** Percentage of "Exceeds Expectations" employees who leave within 12 months
- **Promotion accuracy:** Percentage of promoted employees who succeed in new role (manager assessment at 6 months)
- **Manager effectiveness:** Employee engagement scores for direct reports (measures manager coaching quality)

**Evidence-Based Targets:**
- **Rating validity:** Increase correlation between ratings and objective KPIs from r=0.15 (baseline) to r=0.35 (practitioner evidence shows r=0.22-0.25 is achievable)
- **High-performer retention:** Reduce high-performer turnover from 15% to 10% (fairness perceptions reduce voluntary exits)
- **Promotion success:** ≥80% of promoted employees succeed (improved rating accuracy enables better promotion decisions)

**Measurement Strategy:**
- **Baseline:** Calculate current correlations between ratings and objective performance data
- **Timeline:** 18-24 months (rating validity improves as system matures and managers gain experience)
- **Data Source:** HRIS (performance ratings + promotion data), departmental KPIs (sales, quality metrics), exit interview data
- **Analysis:** Regression analysis controlling for tenure, role, department

**Why This Matters:**
- Demonstrates that standardization improves not just legal compliance but STRATEGIC VALUE of performance management
- Organizational evidence (97% posterior) shows fair processes correlate with better organizational outcomes (r=0.54)
- Provides positive business case beyond risk reduction (addresses executive "compliance vs. performance" concern)

---

### Full Logic Model Visualization

```
INPUTS                ACTIVITIES              OUTPUTS                  OUTCOMES (M)                    IMPACT (Y)
                                                                                                       
$290K investment  →   Design unified      →   200 managers         →   MEDIATOR 1:                →   Y1: Legal Risk
HR team (500 hrs)     rating scale            certified                Appraisal Consistency          - 20-30% ↓ charges
Technology                                    100% digital             - Rating variance ↓50%         - 40% ↓ complaints
Executive sponsor     Manager training        appraisals              - Zero disparate impact         - 25% ↓ settlement costs
                      (8 hours)               Quarterly feedback       (p>0.05)
                                              completed                                                Y2: Org Performance
                      Implement digital   →   Bias monitoring      →   MEDIATOR 2:                →   - Rating validity r↑0.35
                      platform                reviewed monthly         Perceived Fairness             - High-performer retention ↑
                                                                       - Trust: 43% → 60%             - Promotion accuracy ↑80%
                      Deploy bias                                      - "Fair process" ↑17pts        - Manager effectiveness ↑
                      dashboard                                        - Reduces complaints

ASSUMPTIONS:                                   EXTERNAL FACTORS:
- Managers adopt with training support         - Economic conditions stable (no mass layoffs skewing data)
- Digital platform achieves 90% adoption       - No major organizational restructuring during pilot
- Leadership maintains support 12+ months      - EEOC enforcement priorities don't shift dramatically
- Pilot division represents broader org        - No competing HR initiatives draining manager attention
```

---

## Evaluation Framework Overview

### Evaluation Questions

**Primary Evaluation Question:** 
Did the Standardized Performance Appraisal System (SPAS) reduce legal exposure and improve organizational outcomes in the pilot division after 12 months of implementation?

**Secondary Evaluation Questions:**
1. **Implementation fidelity:** Was SPAS implemented as designed (all 5 components at target quality levels)?
2. **Mediator mechanism:** Did SPAS increase appraisal consistency and perceived fairness as hypothesized (X→M path)?
3. **Causal pathway:** Did consistency/fairness improvements lead to reduced complaints and better organizational outcomes (M→Y path)?
4. **Stakeholder acceptance:** Did managers and employees adopt SPAS with 80%+ satisfaction despite initial resistance concerns?
5. **Unintended consequences:** Did SPAS create any negative outcomes (manager frustration, bureaucratic burden, gaming behavior)?
6. **Cost-benefit validation:** Did SPAS achieve expected ROI (benefits ≥ costs) within evaluation period?
7. **Generalizability:** Do pilot results support scaling to full organization, or do context-specific barriers exist?

### Evaluation Approach

**Evaluation Type:** 
- **Formative evaluation** (Months 1-12): Ongoing monitoring of implementation quality, stakeholder feedback, and mediator variables to enable mid-course corrections
- **Summative evaluation** (Month 12-18): Final assessment of outcomes and impact to inform scaling decision

**Evaluation Design:** 
**Quasi-Experimental Pre-Post Comparison with Non-Equivalent Control Group**

- **Treatment group:** Pilot division (250 employees, 25 managers) receives full SPAS implementation
- **Control group:** 2-3 comparable divisions (similar size, demographics, baseline metrics) continue current practices
- **Pre-post design:** Measure outcomes 12 months before and 12 months after SPAS launch
- **Comparison analysis:** (Pilot post - Pilot pre) vs. (Control post - Control pre) = **difference-in-differences estimate**

**Why This Design:**
- ✅ **Causal inference:** Control group accounts for time trends, org-wide changes, external factors
- ✅ **Ethical:** All divisions eventually get SPAS if pilot succeeds (no permanent denial of intervention)
- ✅ **Practical:** Randomized controlled trial not feasible (can't randomize divisions; too disruptive)
- ⚠️ **Limitation:** Non-equivalent groups may have pre-existing differences (mitigate with propensity score matching)

**Mixed Methods Approach:**
- **Quantitative (70%):** Rating distributions, charge/complaint counts, survey scores, objective KPIs → Tests X→M→Y pathways with statistical rigor
- **Qualitative (30%):** Manager interviews (n=15), employee focus groups (n=4), HR observations → Explains HOW and WHY outcomes occurred, identifies unintended consequences

**Timeline:**
- **Baseline data collection:** Months -3 to 0 (before SPAS launch)
- **Formative evaluation:** Months 1-12 (quarterly check-ins)
- **Summative evaluation:** Months 12-18 (final data collection and analysis)
- **Scaling decision:** Month 18 (go/no-go based on evaluation results)

---

## Key Performance Indicators (KPIs)

### Outcome KPIs (What Changed)

#### Primary Outcome KPI: Discrimination Charge Rate

**KPI:** Number of EEOC/state discrimination charges citing performance appraisal as basis, per 1,000 employees
- **Current Baseline:** 32 charges per 1,000 employees annually (8 charges ÷ 250 pilot employees = 32/1000)
- **Target:** 20-30% reduction → **22-25 charges per 1,000 employees** (5-6 charges in pilot division)
- **Timeline:** 12-18 months (recognize lag between implementation and charge filing)
- **Data Source:** EEOC charge database (HR legal tracking), state EEO agency records
- **Collection Method:** Monthly review of all charges filed, code for performance appraisal relevance
- **Measurement Frequency:** Monthly tracking, quarterly analysis
- **Evidence basis:** Organizational evidence (97% posterior, 81K+ charges) shows 20-30% reduction with structured systems

**Success Criteria:**
- ✅ **Exceeds target:** ≥30% reduction (≤22 charges/1000) = Strong evidence for scaling
- ✅ **Meets target:** 20-29% reduction (23-25 charges/1000) = Proceed with scaling
- ⚠️ **Partial success:** 10-19% reduction = Review implementation fidelity, adjust before scaling
- ❌ **Below target:** <10% reduction = Investigate root causes, consider major revisions

---

#### Secondary Outcome KPI 1: Employee Fairness Perceptions

**KPI:** Composite fairness score (6-item scale, 1-5 Likert) measuring procedural and distributive fairness
- **Current Baseline:** 2.9/5.0 (58% favorable, aligned with 43% FEVS trust benchmark adjusted for 5-point scale)
- **Target:** 3.5/5.0 (70% favorable) = +0.6 point increase, +12% favorable
- **Timeline:** 12 months (expect +0.3 at 6 months, +0.6 at 12 months as system matures)
- **Data Source:** Quarterly employee pulse survey (3 items) + annual comprehensive survey (6 items)
- **Collection Method:** Anonymous online survey, 60% response rate target (n=150)
- **Measurement Frequency:** Quarterly pulse + annual comprehensive
- **Evidence basis:** Organizational evidence shows r=0.54 correlation between fairness and outcomes; practitioner evidence shows measurable perception improvements

**Survey Items (6-item composite, α>0.85 reliability target):**
1. "My performance appraisal process is fair and consistent" (procedural fairness)
2. "My manager evaluates my performance based on job-relevant criteria" (procedural fairness)
3. "I receive clear feedback on my performance throughout the year" (procedural fairness)
4. "My performance rating accurately reflects my contributions" (distributive fairness)
5. "Performance ratings in my department are free from bias" (distributive fairness)
6. "I trust the performance appraisal system in this organization" (overall trust)

**Success Criteria:**
- ✅ **Exceeds target:** ≥+0.7 point increase (72%+ favorable) = Exceptional fairness improvement
- ✅ **Meets target:** +0.5-0.6 point increase (68-70% favorable) = On track for scaling
- ⚠️ **Partial success:** +0.3-0.4 point increase = Review mediator mechanisms
- ❌ **Below target:** <+0.3 point increase = Significant concern, investigate barriers

---

#### Secondary Outcome KPI 2: Internal Complaint Rate

**KPI:** Number of HR complaints alleging performance appraisal discrimination/unfairness, per 1,000 employees
- **Current Baseline:** 16 complaints per 1,000 employees annually (4 complaints ÷ 250 pilot employees)
- **Target:** 40% reduction → **9-10 complaints per 1,000 employees** (2-3 complaints in pilot)
- **Timeline:** 6-12 months (internal complaints respond faster than external charges)
- **Data Source:** Employee relations complaint log, manager escalation reports
- **Collection Method:** Monthly review of all formal complaints (written, not verbal concerns)
- **Measurement Frequency:** Monthly tracking, quarterly analysis
- **Evidence basis:** Internal complaints are leading indicator for external charges; faster feedback loop than EEOC charges

**Success Criteria:**
- ✅ **Exceeds target:** ≥50% reduction (≤8/1000) = Strong mediator signal
- ✅ **Meets target:** 35-49% reduction (9-10/1000) = Positive trend
- ⚠️ **Partial success:** 20-34% reduction = Modest improvement
- ❌ **Below target:** <20% reduction = Implementation fidelity or acceptance issue

---

#### Secondary Outcome KPI 3: Rating Validity (Org Performance)

**KPI:** Correlation between performance ratings and objective KPIs (sales, quality, productivity metrics)
- **Current Baseline:** r = 0.15 (weak correlation, indicates ratings not capturing actual performance)
- **Target:** r = 0.30-0.35 (moderate correlation, evidence-based achievable target)
- **Timeline:** 18-24 months (requires system maturity and two full rating cycles)
- **Data Source:** HRIS performance ratings + departmental KPI data (sales targets, quality scores, productivity metrics)
- **Collection Method:** Bivariate correlation analysis, partial correlations controlling for tenure/role
- **Measurement Frequency:** Annual (after each rating cycle)
- **Evidence basis:** Practitioner evidence shows r=0.22-0.25 is achievable with structured systems

**Success Criteria:**
- ✅ **Exceeds target:** r ≥ 0.35 = Ratings are strategically valuable talent data
- ✅ **Meets target:** r = 0.25-0.34 = Meaningful validity improvement
- ⚠️ **Partial success:** r = 0.20-0.24 = Modest improvement
- ❌ **Below target:** r < 0.20 = Ratings still not capturing performance accurately

---

### Process KPIs (How Implementation Went)

#### Implementation Fidelity KPI 1: Manager Training Completion

**KPI:** Percentage of managers who complete 8-hour SPAS certification training and pass competency assessment
- **Target:** 100% completion within 90 days of pilot launch
- **Data Source:** Learning management system (LMS) training records
- **Collection Method:** Automated tracking of course completion + competency assessment scores
- **Measurement Frequency:** Weekly during training rollout, then monthly
- **Success Criteria:** ✅ 100% = Proceed; ⚠️ 90-99% = Identify barriers and re-engage; ❌ <90% = Major implementation failure

**Competency Assessment Criteria (must pass with 80%+ score):**
- Rate 5 realistic employee scenarios using standardized rubric (inter-rater reliability with expert ratings)
- Identify 3 types of bias in sample appraisals (recognition accuracy)
- Document performance example using legal standards (completeness checklist)

---

#### Implementation Fidelity KPI 2: Digital Platform Adoption

**KPI:** Percentage of performance appraisals completed via digital platform (vs. paper/email workarounds)
- **Target:** ≥90% by Month 3, ≥95% by Month 6, 100% by Month 9
- **Data Source:** Digital platform usage analytics + HR records audit
- **Collection Method:** System-generated reports + random audit of 20 appraisals/month
- **Measurement Frequency:** Monthly
- **Success Criteria:** ✅ ≥95% = High fidelity; ⚠️ 85-94% = Investigate workarounds; ❌ <85% = Platform usability issue

---

#### Implementation Fidelity KPI 3: Quarterly Feedback Completion

**KPI:** Percentage of employees who receive documented quarterly feedback sessions (not just annual review)
- **Target:** ≥85% of employees receive ≥3 quarterly sessions in 12-month period
- **Data Source:** Digital platform check-in logs + employee confirmation
- **Collection Method:** System tracking + quarterly employee pulse survey ("Did you have a check-in this quarter?")
- **Measurement Frequency:** Quarterly
- **Success Criteria:** ✅ ≥85% = Meeting design; ⚠️ 70-84% = Manager time constraints; ❌ <70% = Redesign needed

---

#### Stakeholder Engagement KPI: Manager Satisfaction

**KPI:** Manager satisfaction with SPAS (5-point Likert scale, 4-item composite)
- **Target:** ≥3.5/5.0 (70% favorable) despite 44% baseline flexibility concerns
- **Data Source:** Quarterly manager survey (n=25, 100% response rate goal)
- **Collection Method:** Anonymous online survey
- **Measurement Frequency:** Quarterly (track trend: expect dip at Month 3 during adjustment, recovery by Month 9)

**Survey Items:**
1. "The standardized rating scale helps me evaluate performance fairly" (usefulness)
2. "The training prepared me to conduct effective appraisals" (preparation)
3. "The digital platform makes appraisals more efficient" (efficiency)
4. "I have enough flexibility to recognize individual employee contributions" (flexibility concern)

**Success Criteria:** ✅ ≥3.5 = Adoption likely sustainable; ⚠️ 3.0-3.4 = Monitor concerns; ❌ <3.0 = Resistance risk

---

#### Resource Utilization KPI: Budget Variance

**KPI:** Actual implementation costs vs. $75K pilot budget
- **Target:** ≤110% of budget ($82.5K maximum)
- **Data Source:** Finance department project accounting
- **Collection Method:** Monthly expense tracking with variance reporting
- **Measurement Frequency:** Monthly, cumulative review
- **Success Criteria:** ✅ ≤110% = Costs controlled; ⚠️ 111-120% = Review cost drivers; ❌ >120% = Scaling costs at risk

---

### Impact KPIs (Broader Organizational Effects)

#### Organizational Performance KPI: High-Performer Retention

**KPI:** Percentage of "Exceeds Expectations" rated employees who voluntarily leave within 12 months post-rating
- **Current Baseline:** 15% high-performer turnover (SHRM benchmark: 12-18% for high performers)
- **Target:** 10% high-performer turnover (33% reduction from baseline)
- **Timeline:** 18-24 months (requires full rating cycle + retention tracking)
- **Data Source:** HRIS (performance ratings + exit data)
- **Collection Method:** Cohort analysis of Exceeds Expectations employees, track 12-month turnover
- **Evidence basis:** Practitioner evidence shows fairness perceptions correlate with retention (r=0.22-0.25)

**Success Criteria:** ✅ ≤10% = Fairness improvements retaining talent; ⚠️ 11-13% = Modest impact; ❌ >13% = Other factors dominate

---

#### Cultural/Climate KPI: Manager-Employee Trust

**KPI:** Employee engagement score for "I trust my manager" item (5-point Likert scale)
- **Current Baseline:** 3.1/5.0 (62% favorable, typical for organizations with appraisal fairness concerns)
- **Target:** 3.6/5.0 (72% favorable) = +0.5 point increase
- **Timeline:** 12 months
- **Data Source:** Annual employee engagement survey
- **Collection Method:** Anonymous survey, item embedded in broader engagement instrument
- **Evidence basis:** Manager training emphasizes feedback skills and bias awareness, should improve manager-employee relationships

**Success Criteria:** ✅ ≥+0.5 pts = Training improving manager effectiveness; ⚠️ +0.3-0.4 pts = Moderate impact; ❌ <+0.3 pts = Training not transferring to behavior

---

## Data Collection Plan

### Quantitative Data Collection

#### Organizational Data

**Data Type 1: Performance Rating Distributions**
- **Source:** HRIS performance management module (pre and post-implementation)
- **Collection Frequency:** Continuous (extract quarterly)
- **Collection Method:** IT data pull with manager ID, employee demographics (race, gender, age, tenure), department, rating
- **Sample:** All pilot employees (n=250) + control groups (n=500-750) for 12 months pre, 12 months post
- **Analysis Plan:** 
  - Descriptive: Calculate rating distributions (% in each category), average ratings by manager/department
  - Variance analysis: Standard deviation of manager average ratings (consistency measure)
  - Disparate impact: Chi-square tests comparing rating distributions by race, gender, age (H0: no difference)
  - Difference-in-differences: (Pilot post-variance minus Pilot pre-variance) vs. (Control post-variance minus Control pre-variance)
- **Privacy protection:** De-identify employee names; report only aggregate/statistical data

---

**Data Type 2: Legal Claims and Complaints**
- **Source:** EEOC charge database (HR legal tracking), employee relations complaint log
- **Collection Frequency:** Monthly
- **Collection Method:** HR legal team codes all charges/complaints for performance appraisal relevance (yes/no)
- **Sample:** All charges and formal complaints filed during 12 months pre, 12-18 months post
- **Analysis Plan:**
  - Count charges/complaints per 1,000 employees (pilot vs. control)
  - Calculate percent change from baseline
  - Time series analysis (are charges declining month-over-month?)
  - Severity analysis (settlement costs, litigation risk scores)
- **Lag consideration:** Charges filed in Months 7-18 reflect PAST practices (Months -5 to +6), so expect delay in impact

---

**Data Type 3: Objective Performance Metrics**
- **Source:** Departmental KPI databases (sales, quality, productivity)
- **Collection Frequency:** Continuous (extract quarterly)
- **Collection Method:** Finance/operations teams provide KPI data matched to employee IDs
- **Sample:** Employees with objective KPIs available (estimated n=150 of 250 pilot employees)
- **Analysis Plan:**
  - Calculate correlation between performance ratings and objective KPIs (r value)
  - Pre-post comparison: Is r value higher after SPAS implementation?
  - Validity improvement: (Post r) minus (Pre r) = rating validity gain
- **Limitations:** Not all roles have objective KPIs (knowledge workers, support staff); analyze subgroup where available

---

**Data Type 4: Turnover and Retention**
- **Source:** HRIS exit data
- **Collection Frequency:** Monthly
- **Collection Method:** HR extract of all voluntary terminations with performance rating in last review
- **Sample:** All voluntary exits during 12 months pre, 18-24 months post
- **Analysis Plan:**
  - Calculate turnover rate by performance rating category (Exceeds vs. Meets vs. Below)
  - High-performer retention: % of "Exceeds Expectations" who stay 12 months post-rating
  - Exit interview analysis: Code performance/fairness as reason for leaving (qualitative→quantitative)

---

#### Survey Data

**Survey 1: Employee Fairness Perceptions Survey**
- **Target Population:** All pilot division employees (n=250)
- **Sample Size Goal:** 150 responses (60% response rate) for statistical power
- **Frequency:** Quarterly pulse (3 items) + Annual comprehensive (6 items)
- **Collection Method:** Anonymous online survey via Qualtrics/SurveyMonkey, emailed invitation + 2 reminders
- **Items:** 6-item fairness composite (listed in KPI section above) + 2 open-ended for qualitative insights
- **Analysis Plan:**
  - Calculate composite fairness score (α reliability, item-total correlations)
  - Pre-post paired t-test (baseline vs. 12-month)
  - Subgroup analysis: Compare fairness perceptions by demographics, department, manager
  - Correlation with mediator (fairness score vs. actual rating consistency) to test M1→M2 path

---

**Survey 2: Manager Satisfaction & Adoption Survey**
- **Target Population:** All pilot division managers (n=25)
- **Sample Size Goal:** 25 responses (100% response rate - small group, direct request)
- **Frequency:** Quarterly
- **Collection Method:** Anonymous online survey, 10 minutes, administered by external consultant (not HR) to ensure honesty
- **Items:** 4-item satisfaction composite + 6 items on implementation barriers + 2 open-ended
- **Analysis Plan:**
  - Track satisfaction trend (expect Month 3 dip, Month 9 recovery)
  - Identify implementation barriers ("What makes SPAS difficult?") for formative feedback
  - Adoption predictors: Does satisfaction correlate with platform usage, training scores?

---

**Survey 3: Control Group Comparison Survey**
- **Target Population:** Employees in 2-3 control divisions (n=500-750)
- **Sample Size Goal:** 300+ responses (60% response rate)
- **Frequency:** Baseline + 12-month (only 2 timepoints, no quarterly)
- **Collection Method:** Same items as pilot survey to enable direct comparison
- **Analysis Plan:** Difference-in-differences on fairness scores (pilot change vs. control change)

---

### Qualitative Data Collection

#### Manager Interviews (n=15)

**Purpose:** Understand HOW and WHY outcomes occurred, identify unintended consequences, explain statistical patterns

**Sample:** 
- Purposive sample of 15 pilot managers (60% of 25 total)
- Stratified by adoption level (5 high adopters, 5 moderate, 5 low based on platform usage and training scores)
- Ensure diversity by department, tenure, demographics

**Interview Protocol (45 minutes, semi-structured):**
1. **Implementation experience:** "Walk me through your experience learning and using SPAS. What worked well? What was challenging?"
2. **Training effectiveness:** "Did the 8-hour training prepare you? What would you change?"
3. **Rating consistency:** "Has SPAS changed how you evaluate employees? Do you feel more or less confident in your ratings?"
4. **Flexibility concerns:** "Do you feel you have enough flexibility to recognize individual contributions within the standardized framework?"
5. **Unintended consequences:** "Have you observed any negative effects of SPAS? Gaming behavior? Bureaucracy?"
6. **Recommendations:** "If we scale SPAS organization-wide, what should we do differently?"

**Analysis Plan:**
- Thematic coding using grounded theory approach (open coding → axial coding → selective coding)
- Compare themes by adoption level (high vs. low adopters - do they describe different experiences?)
- Use quotes to illustrate quantitative findings (e.g., if satisfaction dips at Month 3, what do interviews reveal about why?)

---

#### Employee Focus Groups (n=4 groups, 8-10 participants each)

**Purpose:** Understand employee perceptions of fairness, identify barriers to trust, validate survey findings

**Sample:**
- 4 focus groups stratified by demographics (1 group per: white employees, employees of color, women, men) to surface potential disparate experiences
- Random selection within strata, voluntary participation
- Total n=32-40 employees across 4 groups

**Focus Group Protocol (90 minutes):**
1. **Fairness perceptions:** "Has your perception of appraisal fairness changed in the past year? Why or why not?"
2. **Quarterly feedback:** "What's your experience with quarterly check-ins? Are they valuable or just another meeting?"
3. **Transparency:** "Do you understand how your performance rating was determined? Do you trust the process?"
4. **Bias concerns:** "Do you believe performance ratings in your department are free from bias? What would increase your confidence?"
5. **Comparison to past:** "How does the new SPAS compare to the old appraisal system? Better, worse, or no different?"

**Analysis Plan:**
- Thematic coding by demographic group (do women/employees of color describe different fairness experiences?)
- Triangulation with survey data (do focus group themes explain survey score patterns?)
- Identify unintended consequences (employee gaming, cynicism, work-arounds)

---

#### HR Observer Notes (Ongoing)

**Purpose:** Document implementation process, capture real-time challenges, inform formative evaluation

**Method:**
- HR project lead maintains monthly observer log documenting:
  - Implementation milestones (on-time? delayed?)
  - Manager/employee questions and concerns (patterns? outliers?)
  - System glitches or technical issues (platform bugs, training gaps)
  - Organizational context changes (restructuring, competing initiatives, leadership transitions)
- Structured template with fields for: Date, Event/Observation, Stakeholder(s), Impact on implementation, Action taken

**Analysis Plan:**
- Review monthly for formative adjustments (if 10 managers ask same question, training needs revision)
- Create implementation timeline showing milestones, barriers, solutions
- Include observer insights in final evaluation report (provides context for outcomes)

---

## Alternative Explanations & Confounding Variables

### Threats to Internal Validity (Is X causing Y, or something else?)

**Threat 1: Selection Bias (Pilot vs. Control Differences)**
- **Risk:** Pilot division may be systematically different from control divisions (e.g., better managers, lower baseline conflict)
- **Evidence:** If pilot already had fewer charges pre-implementation, post-implementation improvement may not generalize
- **Mitigation Strategy:**
  1. **Propensity score matching:** Match pilot and control divisions on baseline characteristics (charge rate, turnover, demographics, manager tenure)
  2. **Pre-trend analysis:** Verify pilot and control had parallel trends in outcomes for 12+ months pre-implementation (if trends diverge BEFORE SPAS, groups not comparable)
  3. **Sensitivity analysis:** Re-run analysis with different control group selections to test robustness

---

**Threat 2: History Effects (Organization-Wide Changes)**
- **Risk:** Other organizational changes during pilot period could explain outcomes (e.g., new CEO, DEI initiative, restructuring)
- **Evidence:** If control divisions show similar outcome improvements, SPAS may not be the cause
- **Mitigation Strategy:**
  1. **Control group comparison:** Difference-in-differences design isolates SPAS effect from time trends
  2. **Timeline documentation:** HR observer log tracks all org-wide policy changes during pilot
  3. **Statistical control:** Include org-wide change indicators as covariates in regression models

---

**Threat 3: Maturation (Time-Dependent Changes)**
- **Risk:** Outcomes improve simply due to passage of time (e.g., natural turnover of problem managers, economic recovery)
- **Evidence:** If outcomes improve linearly regardless of SPAS quality, maturation may be the cause
- **Mitigation Strategy:**
  1. **Dose-response analysis:** Test if higher implementation fidelity (better training scores, higher platform usage) correlates with better outcomes
  2. **Temporal pattern:** Look for improvement acceleration AFTER SPAS launch (not smooth linear trend)

---

**Threat 4: Hawthorne Effect (Attention, Not Intervention)**
- **Risk:** Pilot division improves because they receive special attention/monitoring, not because SPAS is effective
- **Evidence:** If control divisions given equivalent attention show similar improvements, Hawthorne effect is operating
- **Mitigation Strategy:**
  1. **Control for attention:** Provide control divisions with some attention (e.g., non-SPAS training) to isolate intervention effect
  2. **Long-term tracking:** Hawthorne effects typically fade after 6-12 months; track outcomes 18-24 months
  3. **Mechanism testing:** If mediators (consistency, fairness) improve in pilot but not control, attention alone doesn't explain it

---

### Threats to External Validity (Will results generalize to full organization?)

**Threat 1: Volunteer/Selection Bias (Pilot Division Self-Selection)**
- **Risk:** Pilot division may be more motivated/ready than typical divisions (if division was chosen because of manager buy-in)
- **Evidence:** If pilot manager satisfaction is 4.0/5.0 but org-wide manager readiness survey is 2.5/5.0, scaling may fail
- **Mitigation Strategy:**
  1. **Avoid "best case" pilot:** Select pilot division based on representativeness, not readiness
  2. **Readiness assessment:** Survey all divisions on readiness factors; compare pilot to org-wide average
  3. **Phased scaling:** After pilot succeeds, implement in "moderate readiness" division next (not highest readiness) to test robustness

---

**Threat 2: Pilot Fatigue vs. Scaling Fatigue**
- **Risk:** Pilot division tolerates implementation burden because it's temporary; organization-wide rollout may face greater resistance
- **Evidence:** If manager satisfaction declines during scaling (despite pilot success), fatigue is a factor
- **Mitigation Strategy:**
  1. **Manager time tracking:** Measure actual hours spent on SPAS (if >3 hours/appraisal, unsustainable at scale)
  2. **Scaling support:** Provide GREATER training/support during full rollout than during pilot (don't assume pilot lessons transfer automatically)

---

**Threat 3: Context Specificity (Federal Evidence to Private Sector)**
- **Risk:** Organizational evidence (97% posterior) is heavily federal sector; private sector may have different dynamics
- **Evidence:** If pilot achieves 10% charge reduction (not 20-30%), federal-private generalizability is limited
- **Mitigation Strategy:**
  1. **Benchmark to evidence:** Compare pilot outcomes to evidence-based targets; if >50% of targets met, generalizability is reasonable
  2. **Industry comparison:** Seek private sector case studies (practitioner evidence) for benchmarking
  3. **Adjust expectations:** Set conservative targets for full rollout (15% vs. 20-30%) based on pilot learnings

---

## Evaluation Timeline & Milestones

### Month -3 to 0: Baseline Data Collection
- [ ] Extract 12 months of pre-implementation rating distributions, charge/complaint data
- [ ] Administer baseline employee fairness survey (pilot + control, n=450 target)
- [ ] Conduct manager readiness interviews (n=5 exploratory)
- [ ] Calculate baseline KPIs for all outcome measures (document in evaluation tracker)

### Month 1-3: Early Implementation Monitoring
- [ ] Track manager training completion weekly (100% target by Month 3)
- [ ] Monitor digital platform adoption (weekly usage reports)
- [ ] Conduct Month 3 manager satisfaction survey (expect satisfaction dip - formative feedback)
- [ ] Review HR observer notes monthly for implementation barriers

### Month 4-6: Mid-Pilot Checkpoint
- [ ] Administer quarterly employee pulse survey (pilot only, n=150 target)
- [ ] Extract first 6-month rating distribution data (calculate variance, disparate impact tests)
- [ ] Conduct 5 manager interviews (high adopters) to document success factors
- [ ] Review implementation fidelity KPIs; make adjustments if <80% targets met

### Month 7-9: Late Implementation Monitoring
- [ ] Administer Month 9 manager satisfaction survey (expect recovery from Month 3 dip)
- [ ] Track internal complaint data (should see 20-30% reduction by now - leading indicator)
- [ ] Conduct employee focus groups (n=4 groups, 32-40 participants total)
- [ ] Begin EEOC charge tracking (charges filed Months 7-12 reflect implementation period practices)

### Month 10-12: Summative Data Collection (Year 1)
- [ ] Administer 12-month comprehensive employee survey (pilot + control, 6-item fairness composite)
- [ ] Extract full 12-month post-implementation rating distributions
- [ ] Conduct remaining 10 manager interviews (stratified by adoption level)
- [ ] Calculate all outcome KPIs and compare to targets

### Month 13-15: Lag Period Monitoring
- [ ] Continue EEOC charge tracking (Months 13-18 charges reflect late implementation period)
- [ ] Conduct exit interviews with voluntary terminations (high-performer retention KPI)
- [ ] No new surveys (avoid survey fatigue); rely on HR observer notes

### Month 16-18: Final Evaluation & Scaling Decision
- [ ] Complete all statistical analyses (difference-in-differences, correlation, regression)
- [ ] Finalize qualitative analysis (thematic coding of interviews/focus groups)
- [ ] Calculate 3-year ROI projection based on pilot results
- [ ] Prepare evaluation report with scaling recommendation (GO / NO-GO / MODIFY)
- [ ] Present findings to executive leadership for scaling decision

**Scaling Decision Criteria (Month 18):**
- ✅ **GO (Full Rollout):** ≥4 of 5 outcome KPIs meet targets + implementation fidelity ≥85% + manager satisfaction ≥3.5
- ⚠️ **MODIFY (Revise & Re-Pilot):** 2-3 outcome KPIs meet targets + clear barriers identified + solutions proposed
- ❌ **NO-GO (Discontinue):** <2 outcome KPIs meet targets OR unintended negative consequences OR implementation fidelity <70%

---

## Risk Mitigation & Contingency Plans

### Risk 1: Low Survey Response Rates (<60%)
**Impact:** Insufficient statistical power to detect fairness perception changes
**Mitigation:**
- Offer $10 gift card incentive for survey completion
- Conduct survey during work hours (not personal time)
- Executive email emphasizing importance
- Extend survey window from 1 week to 2 weeks
**Contingency:** If <50% response, supplement with focus groups (qualitative) + rely more heavily on objective KPIs (charges, complaints)

### Risk 2: Control Group Contamination
**Impact:** Control divisions adopt SPAS elements informally (managers share materials), reducing difference between groups
**Mitigation:**
- Request control division managers not adopt SPAS during pilot period
- Monitor control divisions for informal adoption (HR observer inquiries)
**Contingency:** If contamination detected, switch to pre-post design only (no control comparison); acknowledge weaker causal inference

### Risk 3: EEOC Charge Data Not Available at Division Level
**Impact:** Cannot calculate pilot-specific charge rate (charges may be org-wide only)
**Mitigation:**
- Work with legal department to code charges by employee division (if not already tracked)
- Use complaint data (internal, division-tracked) as primary outcome if charge data unavailable
**Contingency:** Rely on employee fairness perceptions as primary outcome (highly correlated with charges per organizational evidence r=0.54)

### Risk 4: Pilot Division Experiences Organizational Disruption (Restructuring, Leadership Change)
**Impact:** Outcomes confounded by disruption, not attributable to SPAS
**Mitigation:**
- Select pilot division with stable leadership and no planned restructuring
- HR observer log documents any disruptions for context
**Contingency:** If major disruption occurs (e.g., RIF, merger), extend pilot timeline 6 months OR select new pilot division and restart (sunk cost decision)

---

## Final Confidence Statement on Evaluation Plan

This evaluation plan is designed to:
1. ✅ **Test causal pathway (X→M→Y):** Measures implementation fidelity (X quality), mediators (consistency, fairness), and outcomes (legal risk, org performance)
2. ✅ **Enable causal inference:** Quasi-experimental design with control group, pre-post comparison, difference-in-differences analysis
3. ✅ **Balance rigor and pragmatism:** Mixed methods (70% quantitative for statistical testing, 30% qualitative for contextual understanding)
4. ✅ **Inform scaling decision:** Clear success criteria (≥4 of 5 KPIs meet targets) with GO/NO-GO framework
5. ✅ **Address generalizability:** Pilot tests federal-to-private sector transfer, identifies context-specific barriers
6. ✅ **Mitigate validity threats:** Controls for selection bias, history effects, Hawthorne effect, alternative explanations

**Evaluation confidence level: 90%** - Plan is comprehensive, evidence-based, and feasible within 18-month timeline and existing organizational capacity.

**Primary limitation:** Quasi-experimental design (not RCT) means causal inference is strong but not definitive. Difference-in-differences with propensity score matching is best available design given ethical/practical constraints.
- **Timing:** [When you'll conduct survey - before, during, after implementation]
- **Key Questions:** [Most important questions on the survey]
- **Administration Method:** [How you'll distribute and collect the survey]

**Survey 2:** [Second survey, if conducting multiple]
[Follow same structure]

### Qualitative Data Collection

#### Interview Data
**Interview Type 1: Implementation Experience Interviews**
- **Target Participants:** [Who you'll interview]
- **Number of Interviews:** [How many interviews you plan]
- **Timing:** [When you'll conduct interviews]
- **Key Topics:** [Main questions you'll explore]
- **Interview Method:** [In-person, phone, video, etc.]

**Interview Type 2:** [Second type of interviews]
[Follow same structure]

#### Focus Group Data
**Focus Group:** [Topic/purpose of focus group]
- **Participants:** [Who will participate]
- **Timing:** [When you'll conduct focus group]
- **Key Questions:** [Main topics you'll explore]

#### Observation Data
**What You'll Observe:** [Behaviors, processes, interactions you'll observe]
- **Observation Settings:** [Where you'll observe]
- **Observation Schedule:** [When and how often you'll observe]
- **Documentation Method:** [How you'll record observations]

### Document Review
**Document Type 1:** [First type of documents you'll review]
- **Purpose:** [What these documents will tell you]
- **Collection Method:** [How you'll access these documents]

**Document Type 2:** [Second type of documents]
[Follow same structure]

## Evaluation Timeline

### Pre-Implementation Data Collection [Baseline Period]
**Timeline:** [When you'll collect baseline data]

**Activities:**
- [ ] [First baseline data collection activity]
- [ ] [Second baseline data collection activity]
- [ ] [Third baseline data collection activity]

### During Implementation Monitoring
**Timeline:** [When you'll collect monitoring data during implementation]

**Month 1 Activities:**
- [ ] [First month monitoring activities]

**Month 2 Activities:**
- [ ] [Second month monitoring activities]

**Quarterly Activities:**
- [ ] [Quarterly monitoring activities]

### Post-Implementation Evaluation
**Timeline:** [When you'll conduct final evaluation]

**Immediate Post-Implementation (0-1 month after):**
- [ ] [Immediate follow-up data collection]

**Short-term Follow-up (3-6 months after):**
- [ ] [Short-term follow-up activities]

**Long-term Follow-up (12+ months after):**
- [ ] [Long-term follow-up activities]

## Data Analysis Plan

### Quantitative Analysis

#### Outcome Analysis
**Primary Outcome Analysis:** [How you'll analyze your main success measure]
- **Statistical Approach:** [What statistical methods you'll use]
- **Comparison Strategy:** [Before vs. after, treatment vs. control, etc.]
- **Significance Criteria:** [What counts as meaningful change]

**Secondary Outcome Analysis:** [How you'll analyze other outcome measures]
[Follow same structure]

#### Trend Analysis
**How You'll Examine Trends Over Time:** [Method for analyzing changes over time]
**Patterns You'll Look For:** [What kinds of trends would be meaningful]

### Qualitative Analysis

#### Thematic Analysis
**Interview Analysis Approach:** [How you'll analyze interview data]
**Coding Strategy:** [How you'll organize and categorize qualitative data]
**Theme Identification:** [How you'll identify key themes]

#### Content Analysis
**Document Analysis Approach:** [How you'll analyze documents]
**Observation Analysis:** [How you'll analyze observational data]

### Mixed Methods Integration
**How You'll Combine Quantitative and Qualitative Findings:** [Strategy for integrating different types of data]
**Triangulation Approach:** [How you'll use multiple data sources to validate findings]

## Success Criteria and Interpretation

### Success Thresholds

#### Must-Achieve Criteria (Implementation considered successful only if these are met)
**Criterion 1:** [First essential success criterion]
- **Measurement:** [How you'll measure this]
- **Threshold:** [Specific level that must be achieved]

**Criterion 2:** [Second essential success criterion]
[Follow same structure]

#### Should-Achieve Criteria (Important for full success but not make-or-break)
**Criterion 1:** [First important success criterion]
- **Measurement:** [How you'll measure this]
- **Target:** [Desired level of achievement]

**Criterion 2:** [Second important success criterion]
[Follow same structure]

#### Could-Achieve Criteria (Nice-to-have outcomes)
**Criterion 1:** [First nice-to-have success criterion]
[Follow same structure]

### Interpretation Framework

#### Strong Success Indicators
[What combination of results would indicate strong success]

#### Moderate Success Indicators
[What combination of results would indicate moderate success]

#### Weak Success or Failure Indicators
[What combination of results would indicate the solution didn't work well]

#### Unintended Consequences Assessment
**Positive Unintended Consequences:** [Good things that might happen that you didn't plan for]
**Negative Unintended Consequences:** [Problems that might arise that you need to watch for]
**Monitoring Strategy:** [How you'll detect unintended consequences]

## Stakeholder Feedback Integration

### Feedback Collection Strategy

#### Formal Feedback Mechanisms
**Mechanism 1:** [First formal way stakeholders can provide feedback]
- **Frequency:** [How often this feedback is collected]
- **Participants:** [Who provides this feedback]
- **Process:** [How feedback is collected and processed]

**Mechanism 2:** [Second formal feedback mechanism]
[Follow same structure]

#### Informal Feedback Mechanisms  
**Mechanism:** [How you'll capture informal feedback]
**Documentation:** [How you'll record and use informal feedback]

### Feedback Integration Process
**How You'll Incorporate Stakeholder Feedback:** [Process for using feedback to improve implementation]
**Decision-Making Process:** [How feedback will influence decisions about continuing/modifying the solution]

## Evaluation Reporting

### Reporting Schedule
**Monthly Reports:** [What you'll include in monthly progress reports]
**Quarterly Reports:** [What you'll include in quarterly reports]
**Annual Report:** [What you'll include in comprehensive annual evaluation]

### Report Audiences
**Leadership Reports:** [What leaders need to know and when]
**Staff Reports:** [What staff need to know about progress and results]
**Stakeholder Reports:** [What other stakeholders need to know]
**External Reports:** [Any external reporting requirements]

### Report Content Framework
**Executive Summary:** [Key points for busy executives]
**Progress Against Goals:** [Status on achieving targets]
**Key Findings:** [Most important discoveries from evaluation]
**Lessons Learned:** [What you've learned about implementation]
**Recommendations:** [What should be done next based on evaluation results]

## Continuous Improvement Process

### Learning Integration
**How You'll Use Evaluation Results for Ongoing Improvement:** [Process for applying what you learn]
**Adjustment Mechanisms:** [How you'll modify the solution based on evaluation findings]

### Knowledge Management
**Documentation Strategy:** [How you'll document lessons learned]
**Knowledge Sharing:** [How you'll share learnings with others who might implement similar solutions]

### Sustainability Assessment
**Long-term Viability Evaluation:** [How you'll assess whether the solution can be sustained long-term]
**Resource Requirement Assessment:** [How you'll evaluate ongoing resource needs]
**Adaptation Planning:** [How you'll plan for future modifications based on changing conditions]

---
INSTRUCTIONS:
1. Design evaluation to answer the most important questions about your solution's effectiveness
2. Include both quantitative metrics and qualitative insights
3. Plan for both implementation monitoring and outcome evaluation
4. Build in mechanisms for continuous improvement based on evaluation findings
5. Consider different stakeholder information needs in your reporting plan
